# -*- coding: utf-8 -*-
"""Ari_of_stack_over_flow_tag_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11LveJJEMgdn3mLK8sO_ydpMp1eqAnJRr

### IMPORTING LIBRARIES
"""

import numpy as np
import pandas as pd
import string
import nltk

from google.colab import drive
drive.mount('/content/drive')

"""### READING THE DATA"""

question= pd.read_csv('/content/drive/MyDrive/stackoverflow tag/Questions.csv', encoding='latin')
tags= pd.read_csv('/content/drive/MyDrive/stackoverflow tag/Tags.csv', encoding='latin')

question.head()

tags.head()

print(question.shape,tags.shape)

print(question.Id.nunique(), tags.Id.nunique())

"""### MERGING THE DATA FRAMES"""

tags['Tag']= tags['Tag'].astype(str)
grouped_tags = pd.DataFrame(tags.groupby("Id")['Tag'].apply(lambda tags: ' '.join(tags)))
grouped_tags.columns= ['Tag']
print(grouped_tags.head())
grouped_tags['Tag']= grouped_tags['Tag'].astype(str)
grouped_tags['Tag']= grouped_tags['Tag'].apply(lambda x: x.split())
# grouped_tags= grouped_tags.to_frame()
grouped_tags= grouped_tags.sort_values(by='Id')
print(grouped_tags.head())

grouped_tags1 = grouped_tags

grouped_tags = grouped_tags1

flat_list = [item for sublist in grouped_tags['Tag'].values for item in sublist]

keywords = nltk.FreqDist(flat_list)

keywords = nltk.FreqDist(keywords)

frequencies_words = keywords.most_common(100)
tags_features = [word[0] for word in frequencies_words]
tags_features

def most_common(tags):
    tags_filtered = []
    for i in range(0, len(tags)):
        if tags[i] in tags_features:
            tags_filtered.append(tags[i])
    return tags_filtered

grouped_tags['Tag'] = grouped_tags['Tag'].apply(lambda x: most_common(x))
grouped_tags['Tag'] = grouped_tags['Tag'].apply(lambda x: x if len(x)>0 else None)

grouped_tags.dropna(subset=['Tag'], inplace=True)

print(grouped_tags.shape)

"""1. Merging Question and grouped_answer dataframes to get df
2. Merging df and grouped_answer dataframes to get df
"""

grouped_tags['Ids']= grouped_tags.index
question.columns= ['Ids', 'OwnerUserId', 'CreationDate', 'ClosedDate', 'Score', 'Title',
       'Body']
question= question.sort_values(by='Ids')
df= pd.merge(question,grouped_tags, on='Ids')

df.head()

df.shape

"""### REMOVING UNNECESSARY VARIABLES"""

df.drop(columns=['Ids', 'OwnerUserId', 'CreationDate', 'ClosedDate'],inplace=True)
df.head()

"""### FILTERING DATA BASED ON SCORE AND MOST FREQUENTLY USED TAGS"""

print(df.Score.min(), df.Score.max())

# z= df['Tag'].value_counts().sort_values(ascending=False)
# z.index

# df1= df.groupby(by='Tag')['Tag'].count().sort_values(ascending=False).to_frame()
# df1.columns= ['Tag_count']
# df1['Tags']=df1.index

# df.columns= ['Score', 'Title', 'Body', 'Tags']
# df1= pd.merge(df,df1,how='left',on='Tags')
# df1.head()

# df1= df1[df1['Tag_count']>=500]
df1= df[df['Score']>6]
df1.shape

"""For better prediction we will be using only those tags which have the score more than 6. Low scores mean that the question is either erroneous or does not have sufficient information."""

type(df1['Tag'])

df1['Tag']

# df1.Tag.value_counts().sort_values(ascending=False)

"""### CHECKING FOR MISSING VALUES"""

print(df1.isnull().sum())

print('Shape of df1:',df1.shape)

"""### CLEANING THE TEXT FOR TITLE AND BODY

1. Removing punctuation
2. Removing HTML tags (if required)
3. Changing text into lowercase
4. Splitting the text into words
5. Removing stopwords

#### PUNCTUATION & HTML TAGS REMOVAL, LOWERCASE, WORD TOKENIZATION
"""

import string
def remove_punctuation(text):
    for punctuation in string.punctuation:
        text= text.replace(punctuation,'')
    return text

df1['Title']= df1['Title'].astype(str)

df1['Title1']= df1['Title'].apply(remove_punctuation)
df1['Title1']=df1['Title1'].str.lower()
df1['Title1']= df1['Title1'].str.split()
df1['Title1'].head()

df1['Body']= df1['Body'].astype(str)
import re

df1['Body1']= df1['Body'].apply(lambda x: re.sub('<[^<]+?>','',x))
df1['Body1'].head()

df1['Body1']= df1['Body1'].apply(remove_punctuation)
df1['Body1']=df1['Body1'].str.lower()
df1['Body1']= df1['Body1'].str.split()
df1['Body1'].head()

"""#### LEMMATIZATION"""

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')
lematizer= WordNetLemmatizer()

def word_lemmatizer(text):
    lem_text=[lematizer.lemmatize(i) for i in text]
    return lem_text

df1['Title1']= df1['Title1'].apply(lambda x: word_lemmatizer(x))

df1['Body1']= df1['Body1'].apply(lambda x: word_lemmatizer(x))

"""#### STOPWORD REMOVAL"""

nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords=stopwords.words('english')

df1['Body1']=df1['Body1'].apply(lambda x: " ".join([w for w in x if w not in stopwords]))
df1['Body1'].head(5)

df1['Title1']=df1['Title1'].apply(lambda x: " ".join([w for w in x if w not in stopwords]))
df1['Title1'].head(5)

"""### FINAL DATAFRAME AFTER TEXT CLEANING"""

df1= df1.drop(['Title', 'Body','Score'],axis=1)
df1.head()

"""### TF-IDF VECTORIZATION"""

from sklearn.feature_extraction.text import TfidfVectorizer

df1['Title1']= df1['Title1'].astype(str)
vectorizer = TfidfVectorizer()
X1 = vectorizer.fit_transform(df1['Title1'].str.lower())

df1['Body1']= df1['Body1'].astype(str)
vectorizer = TfidfVectorizer()
X2 = vectorizer.fit_transform(df1['Body1'].str.lower())

"""### CHANGING CATEGORICAL VARIABLES INTO NUMERIC"""

# from sklearn.preprocessing import LabelEncoder
# le= LabelEncoder() 
# df1['Tags']= le.fit_transform(df1['Tags'])

from sklearn.preprocessing import MultiLabelBinarizer

multilabel= MultiLabelBinarizer()

y= multilabel.fit_transform(df1['Tag'])

df1['Tag']

y

"""### SPLITTING THE DATASET INTO TRAIN AND TEST SET"""

y.shape

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X2, y, test_size=0.30, random_state=42)
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

"""### APPLYING DIFFERENT ALGORITHMS"""

from sklearn.metrics import classification_report

lr = LogisticRegression(C=20)

# Creating the model on Training Data

from sklearn.metrics import hamming_loss
from sklearn.multiclass import OneVsRestClassifier
clf = OneVsRestClassifier(lr)
model=clf.fit(x_train,y_train)
prediction=model.predict(x_test)
# Printing the Overall Accuracy of the model
from sklearn import metrics
print(metrics.multilabel_confusion_matrix(y_test, prediction))
print(classification_report(y_test,prediction))
F1_Score=metrics.classification_report(y_test, prediction).split()[-2]
print('Accuracy of the model:', F1_Score)
hamming_l = hamming_loss(y_test, prediction)
print('Hamming loss of the model: ', hamming_l)

from sklearn.linear_model import SGDClassifier
from sklearn import metrics
sgd = SGDClassifier()
from sklearn.multiclass import OneVsRestClassifier
clf = OneVsRestClassifier(sgd)
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
print(classification_report(y_test,y_pred))
print(metrics.multilabel_confusion_matrix(y_test, y_pred))
F1_Score=metrics.classification_report(y_test, y_pred).split()[-2]
print('Accuracy of the model:', F1_Score)
hamming_l = hamming_loss(y_test, y_pred)
print('Hamming loss of the model: ', hamming_l)

from sklearn.svm import LinearSVC
sv= LinearSVC()
from sklearn.multiclass import OneVsRestClassifier
clf = OneVsRestClassifier(sv)
model = clf.fit(x_train,y_train)
y_pred= model.predict(x_test)
print(classification_report(y_test,y_pred))
F1_Score=metrics.classification_report(y_test, y_pred).split()[-2]
print('Accuracy of the model:', F1_Score)
hamming_l = hamming_loss(y_test, y_pred)
print('Hamming loss of the model: ', hamming_l)

"""Implementation of model"""

x = ['which is better java, c++ or javascript.']
xt = vectorizer.transform(x)
clf.predict(xt)
multilabel.inverse_transform(clf.predict(xt))

x = ['how to implement sql queries in python']
xt = vectorizer.transform(x)
clf.predict(xt)
multilabel.inverse_transform(clf.predict(xt))

import matplotlib.pyplot as plt
plt.figure(figsize=(5, 5))
classes = ['lr', 'sgd', 'svc']
index = np.arange(len(classes))
# ham = [0.037142257658413766, 0.04078892152748636, 0.0358581619806966]
f1 = [0.54, 0.42, 0.55]
width = 0.2
plt.bar(classes, f1, width, label = 'F1 score')
# plt.bar(index+width, ham, width,  label = 'Hamming Loss')
plt.legend()
plt.show()

ham = [0.037142257658413766, 0.04078892152748636, 0.0358581619806966]
plt.bar(classes, ham, width,  label = 'Hamming Loss')
plt.legend()
plt.show()